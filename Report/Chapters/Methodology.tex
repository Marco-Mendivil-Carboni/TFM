\chapter{Metodología}
\label{cap:methodology}

\section{Dinámica Molecular}

En este trabajo realizaremos concretamente simulaciones de dinámica molecular \cite{Rapaport2004}, que son aquellas en las que se resuelven numéricamente las ecuaciones del movimiento clásicas. Su nombre proviene de que, en virtud de la aproximación de Born-Oppenheimer, este método se puede y se suele utilizar para estudiar el movimiento de los núcleos atómicos en las moléculas. En las llamadas simulaciones MD \textit{all-atom}, cada una de las partículas representa uno de estos núcleos pero cuando se quieren simular macromoléculas como en este trabajo es habitual usar modelos \textit{coarse-grained} en los que cada partícula representa un grupo de átomos, como el que hemos descrito en el capítulo \ref{cap:chromatin}. Al reducir los grados de libertad son posibles tiempos de simulación mucho mayores, a costa de perder algo de detalle.

Pero además la cromatina (al igual que cualquier otra biomolécula) está inmersa en un solvente, el agua, que tiene efectos importantes sobre el comportamiento del soluto. Ahora bien, introducir explícitamente en las simulaciones las moléculas del solvente, que es de escaso interés en sí mismo, requiere de una gran cantidad de recursos y no suele ser necesario. En su lugar este se suele tener en cuenta de forma implícita \cite{Leach2001}, incorporando solo sus efectos sobre las partículas de nuestro modelo: la modulación de los potenciales efectivos que actúan sobre ellas, la resistencia viscosa a su movimiento y las colisiones aleatorias de estas con las partículas del medio. El método más empleado para lograrlo es la dinámica de Langevin, que consiste simplemente en añadir a las ecuaciones del movimiento de la dinámica molecular una fuerza de fricción proporcional a la velocidad y una fuerza aleatoria que representa el efecto de estas colisiones. Es decir, que cada una de las partículas del sistema obedece la que se conoce como una ecuación de Langevin,
\begin{equation}
    \label{eq:Langevin}
    m_i\frac{d^2\bar{r}_i}{dt^2}=-\xi_i\frac{d\bar{r}_i}{dt}-\bar{\nabla}_iV\left(\{\bar{r}_j\}\right)+\bar{\zeta}_i(t)
\end{equation}
donde $m_i$ es la masa de la partícula $i$, $\xi_i$ es su coeficiente cinético y $\bar{\zeta}_i(t)$ es la fuerza aleatoria sobre ella. Los coeficientes cinéticos dependen del medio y la geometría de las partículas y los estimaremos más adelante. Por último las fuerzas aleatorias dependen del medio y de los coeficientes cinéticos tal y como vamos a ver a continuación.

En realidad $\bar{\zeta}_i(t)$ no son funciones del tiempo en el sentido matemático habitual sino procesos estocásticos gaussianos con dos propiedades fundamentales:
\begin{equation}
    \label{eq:random_forces_properties}
    \langle \zeta_i^\alpha(t) \rangle=0 \quad \text{y} \quad \langle \zeta_i^\alpha(t)\zeta_j^\beta(t') \rangle=2\xi_ik_BT\delta_{ij}\delta_{\alpha\beta}\delta(t-t') \quad \forall \ i,j=1,\dots,N \land \alpha,\beta=x,y,z.
\end{equation}
De esta forma, la dinámica de Langevin al introducir los efectos del solvente actúa simultáneamente como un termostato, permitiendo controlar la temperatura del sistema. Se puede demostrar \cite{Coffey2012} que esta ecuación efectivamente genera trayectorias que reproducen la distribución de probabilidad canónica. De hecho la varianza de las fuerzas aleatorias se puede deducir del teorema de equipartición. Las simulaciones de dinámica molecular sin estas modificaciones, en cambio, mantienen la energía constante, simulando el formalismo microcanónico. En los sistemas fuera del límite termodinámico (como los que se suelen estudiar con estas técnicas) los distintos formalismos de la mecánica estadística no son equivalentes. Por ello, la dinámica de Langevin se usa muy frecuentemente dado que los sistemas reales no suelen ser aislados sino que están en contacto con un baño térmico.

Además de los parámetros que introdujimos en el capítulo \ref{cap:chromatin} ahora debemos estimar el valor del coeficiente cinético de las partículas, $\xi$. Las propiedades de equilibrio no dependen de él (siempre que no sea nulo) pero la dinámica sí que lo hace. Para estimar su valor nos valdremos de la fórmula de Stokes (aunque esta realmente se aplica únicamente a esferas aisladas), $\xi=3\pi\eta d=0.28$ pg/$\mu$s, donde $\eta$ es la viscosidad del agua y $d$ el diámetro de la partícula (en nuestro caso $\sigma$). Esto significa que el número de Reynolds en este sistema es $\text{Re}=\rho v_ul_u/\eta=1.7\cdot 10^{-5}$, es decir, estamos en el régimen de bajo número de Reynolds y en la dinámica dominan los efectos disipativos frente a los inerciales \cite{Purcell1977}. Debido a esto podemos despreciar el término inercial en \eqref{eq:Langevin} y utilizar la llamada dinámica Browniana cuyas ecuaciones del movimiento son,
\begin{equation}
    \label{eq:Brownian}
    \xi_i\frac{d\bar{r}_i}{dt}=-\bar{\nabla}_iV\left(\{\bar{r}_j\}\right)+\bar{\zeta}_i(t).
\end{equation}
Si bien es cierto que podríamos emplear las ecuaciones del movimiento completas para hacer simulaciones en el régimen sobreamortiguado, el paso temporal tan pequeño que sería necesario para que las simulaciones fueran precisas (o simplemente el algoritmo convergiera) las haría prohibitivamente largas.

Para resolver ecuaciones diferenciales estocásticas como \eqref{eq:Brownian} no se pueden utilizar los mismos métodos que para las ecuaciones diferenciales ordinarias, pero afortunadamente solo hay que modificarlos ligeramente para incluir correctamente los términos que son procesos estocásticos. Un algoritmo basado en el método de Runge-Kutta de segundo orden (también llamado método de Heun) \cite{Toral2014} aplicado a la ecuación de la dinámica Browniana toma la siguiente forma,
\begin{equation}
    \label{eq:RK_method}
    \begin{aligned}
        (\bar{r}_i)'_{n}  & = (\bar{r}_i)_n+\frac{1}{\xi_i}(\bar{f}_i)_ndt+\frac{1}{\xi_i}(\bar{u}_i)_n\sqrt{dt}                                        \\
        (\bar{r}_i)_{n+1} & = (\bar{r}_i)_n+\frac{1}{2}\frac{1}{\xi_i}\left((\bar{f}_i)'_n+(\bar{f}_i)_n\right)dt+\frac{1}{\xi_i}(\bar{u}_i)_n\sqrt{dt}
    \end{aligned}
\end{equation}
donde
\begin{equation}
    \label{eq:RK_method_forces}
    (\bar{f}_i)_n=-\bar{\nabla}_iV(\{(\bar{r}_j)_n\}) \quad \text{y} \quad (\bar{f}_i)'_n=-\bar{\nabla}_iV(\{(\bar{r}_j)'_n\})
\end{equation}
y cada componente de $(\bar{u}_i)_n$ es un número (pseudo)aleatorio con distribución normal, media nula y varianza $2\xi_ik_BT$.

\section{Programación en GPUs}

Este simulaciones tendrán una gran cantidad de partículas ($30386$) y por lo tanto serán muy costosas computacionalmente, lo cual hasta hace poco había impedido estudiar el núcleo celular completo mediante simulaciones numéricas. Por esta razón es necesario aprovechar al máximo la potencia de cálculo del \textit{hardware} del que disponemos. Esta necesidad, común a muchas tareas de investigación, ha incentivado la computación de propósito general en GPUs (abreviada GPGPU, del inglés \textit{General-Purpose computing on Graphics Processing Units}). Las GPUs son procesadores diseñados para la generación de gráficos 3D pero dadas sus características también son muy apropiadas para propósitos científicos como la realización de simulaciones. Prueba de ello son los numerosos programas profesionales de dinámica molecular (GROMACS, NAMD, CHARMM, etc.) que están acelerados por GPU.

La característica de las GPUs (además de su precio) que las hace tan atractivas para estas aplicaciones es su gran paralelismo, es decir, su capacidad para ejecutar muchas instrucciones simultáneamente. Las simulaciones de dinámica molecular se pueden beneficiar en gran medida del paralelismo ya que, en cada iteración, para hallar las nuevas coordenadas de una cierta partícula no necesitamos las nuevas coordenadas del resto, solo las viejas. En otras palabras, con un programa paralelo de simulación en una GPU podemos actualizar a la vez las coordenadas de una gran cantidad de partículas (aunque quizás no todas) ahorrando un tiempo de cálculo considerable\footnote{Las CPUs tienen frecuencias de reloj más altas que las GPUs por lo que pueden ser más rápidas si no hay muchas operaciones que realizar simultáneamente, esto es, si hay muy pocas partículas.}.

Aunque hay muchas herramientas para el desarrollo de \textit{software} GPGPU el estándar, especialmente en el ámbito científico, es CUDA \cite{Wilt2013} (\textit{Compute Unified Device Architecture}): una plataforma de computación en paralelo creada por NVIDIA que permite ejecutar en GPUs de esta empresa programas escritos utilizando CUDA C, una variación del lenguaje C.

\section{Cálculo del volumen excluido}

Como anticipamos en el capítulo \ref{cap:introduction} la interacción de volumen excluido será la más costosa computacionalmente y por ello creemos conveniente mostrar el algoritmo que se ha empleado para el cálculo de las fuerzas debidas a la misma. Por simplicidad puede ser tentador implementar esta interacción de forma \textit{naive} calculando las distancias entre todas las parejas de partículas, pero este método es $\mathcal{O}(N^2)$ y hay algoritmos, como el que vamos a ver, cualitativamente mejores. En concreto el que describiremos a continuación se ha adaptado de \cite{Green2010}, donde se dan indicaciones muy útiles para su implementación en CUDA.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[color1,fill=color1,fill opacity=0.25] (1.7,1.2) circle (0.5) node [opacity=1] {0};
        \draw[color1,fill=color1,fill opacity=0.25] (2.9,2.1) circle (0.5) node [opacity=1] {1};
        \draw[color1,fill=color1,fill opacity=0.25] (2.1,2.4) circle (0.5) node [opacity=1] {2};
        \draw[color1,fill=color1,fill opacity=0.25] (0.9,2.1) circle (0.5) node [opacity=1] {3};
        \draw[color1,fill=color1,fill opacity=0.25] (2.8,2.8) circle (0.5) node [opacity=1] {4};
        \draw[color1,fill=color1,fill opacity=0.25] (0.6,2.8) circle (0.5) node [opacity=1] {5};
        \foreach \i in {0,...,4} {\draw[black] (\i,0) -- (\i,4);}
        \foreach \j in {0,...,4} {\draw[black] (0,\j) -- (4,\j);}
        \foreach \i in {0,...,3} {\foreach \j [evaluate=\num using int(\i+4*\j)] in {0,...,3} {\node[above right] at (\i,\j) {\num};}}
    \end{tikzpicture}
    \caption{Diagrama esquemático de una malla uniforme en dos dimsensiones  con 16 celdas y 6 partículas.}
    \label{fig:grid-diagram}
\end{figure}

La idea clave detrás del algoritmo que vamos a utilizar es que la interacción de volumen excluido es local (tiene un radio de corte). Esto quiere decir que solo hace falta calcular la distancia de cada partícula con sus vecinas y se puede acelerar mucho la búsqueda de estas partículas vecinas subdividiendo el espacio de simulación. En concreto utilizaremos una malla uniforme, que es la subdivisión espacial más sencilla que existe. Una malla uniforme subdivide el espacio de simulación (la esfera del núcleo en nuestro caso) en celdas (cúbicas) del mismo tamaño. Esta malla se puede utilizar de dos formas, o bien se asigna a cada partícula una única celda en función de la posición de su centro sin tener en cuenta sus dimensiones, o se encuentran todas las celdas que pueden interaccionar con la partícula y se almacenan los índices de todas estas celdas. El primer método es considerablemente más sencillo y probablemente tenga un rendimiento muy similar por lo que será el que utilizaremos.

Para simplificarlo todavía más utilizaremos una malla en la que el tamaño de las celdas es $2\sigma$, el radio de corte del potencial de Wang-Frenkel atractivo, de forma que cada una de las partículas solo podrá interaccionar con las partículas de su propia celda y las celdas vecinas (colindantes). En tres dimensiones, como en nuestro caso, habrá 27 celdas vecinas y por lo tanto para calcular las fuerzas sobre una partícula solo deberemos calcular la distancia con las partículas que se encuentran en estas celdas, en lugar de todas las partículas de la simulación.

Sin embargo para poder hacer esto debemos tener una lista de las partículas que hay en cada celda. Aunque el proceso necesario para obtener esta lista no es demasiado complicado para ilustrarlo mejor consideremos el caso sencillo en dos dimensiones que se representa en \ref{fig:grid-diagram}. Empezaremos por crear una lista A de tuplas que contengan los índices de cada partícula y los índices de la celda en la que se encuentran (que son muy sencillos de calcular), en este caso: A = [(0,5),(1,10),(2,10),(3,8),(4,10),(5,8)]. A continuación ordenaremos esta lista en función del índice de la celda (en lugar del índice de la partícula) obteniendo en este caso: A = [(0,5),(3,8),(5,8),(1,10),(2,10),(4,10)]. Finalmente recorreremos esta lista y almacenaremos en otra lista B el principio y final de cada celda en la lista A ordenada, luego en este caso: B = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,1),(1,1),(1,1),(1,3),(3,3),(3,6),(6,6),(6,6),(6,6),(6,6),(6,6)]. De forma que para calcular, por ejemplo, las interacciones de la partícula 5, que se encuentra en la celda 8, deberemos calcular su distancia con las partículas de las celdas 4, 5, 8, 9, 12 y 13. Gracias al último paso sabemos que solo tenemos que calcular la distancia a las partículas en las posiones 0, 1 y 2 de la lista A ordenada, es decir, las partículas 0, 3 y 5 (que descartamos al ser ella misma).

Todo este proceso se repetirá con cada paso de tiempo y su parte más costosa computacionalmente será la ordenación de la lista A que sin embargo es $\mathcal{O}(N)$ y por lo tanto es mucho más veloz que la implementación \textit{naive} para los valores de $N$ elevados que vamos a utilizar. En la figura \ref{fig:performance} se ve como efectivamente el tiempo de ejecución del programa completo es proporcional a $N$ (más una constante debido a otros factores que impiden tiempos menores con valores de $N$ bajos).

\begin{figure}[t]
    \centering
    \includegraphics{../Plots/performance.pdf}
    \caption{Tiempo de ejecución de un paso de tiempo de la simulación en función del número de partículas en dos GPUs distintas.}
    \label{fig:performance}
\end{figure}
